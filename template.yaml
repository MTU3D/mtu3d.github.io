theme: classic # default || classic || dark
organization: BIGAI
twitter: '@BIGAI'
title: "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation"
journal: "Arxiv"
resources:
  paper: # https://openreview.net/
  arxiv: None
  code: None
  video: https://www.youtube.com/embed/MmjwnIh54oo?si=eTipXVmsCa2MP76B
  demo: 
  huggingface: 
description: Project page for MTU3D

image: public/mut3d-teaser.pdf
url: https://denkiwakame.github.io/academic-project-template
speakerdeck: # speakerdeck slide ID
authors:
  - name: Ziyu Zhu
    affiliation: [1, 2]
    position: Senior Researcher
  - name: Xilin Wang
    affiliation: [2, 4]
    position: Researcher
  - name: Yixuan Li
    affiliation: [2, 3]
    position: Researcher
  - name: Zhuofan Zhang
    affiliation: [1, 2]
    position: Researcher
  - name: Xiaojian Ma
    affiliation: [2]
    position: Researcher
  - name: Yixin Chen
    affiliation: [2]
    position: Researcher
  - name: Baoxiong Jia
    affiliation: [2]
    position: Researcher
  - name: Wei Liang
    affiliation: [3]
    position: Researcher
  - name: Qian Yu
    affiliation: [4]
    position: Researcher
  - name: Zhidong Deng
    affiliation: [1, ✉️]
    position: Senior Researcher
  - name: Siyuan Huang
    affiliation: [2, ✉️]
    position: Researcher
  - name: Qing Li
    affiliation: [2, ✉️]
    position: Researcher
affiliations:
  - Tsinghua University
  - BIGAI, China
  - Beijing Institute of Technology
  - Beihang University
meta:
bibtex: >
  @article{doe2024superai,
    author    = {Jane Doe and John Smith},
    title     = {Unleashing the Power of Super AI: Transforming the Future of Technology},
    journal   = {Journal of Superintelligent Systems},
    year      = {2024},
    volume    = {99},
    number    = {1},
    pages     = {1-42},
    month     = {January},
    keywords  = {Super AI, Machine Learning, Artificial Intelligence, Technological Innovation},
    doi       = {10.9999/jsis.2024.001},
    url       = {https://www.example.com/superai-article},
    note      = {This paper sets the benchmark for future AI research and applications.}
  }

teaser: mtu3d-teaser.png
abstract: |
  Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. 
  Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. 
  To address this limitation, we introduce **M**ove **t**o **U**nderstand (**MTU3D**), a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment.
   This is achieved by three key innovations: 1) *Online query-based representation learning*, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction.
    2) *A unified objective for grounding and exploring*, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 
    3) *End-to-end trajectory learning* that combines Vision-Language-Exploration pre-training over a  million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\%, 27\%, 11\%, and 3\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively.
    MTU3D's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. The deployment on a real robot demonstrates MTU3D's effectiveness in handling real-world data. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.

body:
  - title: Contribution
    text: |
      - We present **MTU3D**, bridging visual grounding and exploration for *efficient and versatile* embodied navigation.
      - We propose a **unified objective** that jointly optimizes grounding and exploration, leveraging their complementary nature to enhance overall performance.
      - We propose a novel **Vision-Language-Exploration** training scheme, leveraging *large-scale trajectories* from simulation and real-world data.
      - Extensive experiments validate the effectiveness of our approach, demonstrating significant improvements in exploration efficiency and grounding accuracy across *open-vocabulary* navigation, *multi-modal lifelong* navigation, *task-oriented sequential* navigation, and *active embodied question-answering* benchmarks.
     
  - title: Method
    text: |
      <img src="mtu3d-model_motion.gif" alt="MTU3D Model" style="width:100%; height:auto;">

  - title: Quantitative results on HM3D-OVON
    text: |
      | **Method**       | **Val Seen**          |                     | **Val Seen Syn** |                     | **Val Unseen**      |                     |
      |-------------------|-----------------------|---------------------|--------------------------|---------------------|---------------------|---------------------|
      |                   | **SR (↑)**           | **SPL (↑)**         | **SR (↑)**              | **SPL (↑)**         | **SR (↑)**          | **SPL (↑)**         |
      | **RL**           | 39.2                 | 18.7                | 27.8                    | 11.7                | 18.6                | 7.5                 |
      | **DAgRL**        | 41.3                 | 21.2                | 29.4                    | 14.4                | 18.3                | 7.9                 |
      | **VLFM**         | 35.2                 | 18.6                | 32.4                    | 17.3                | 35.2                | 19.6                |
      | **Uni-NaVid**    | 41.3                 | 21.1                | 43.9                    | **21.8**            | 39.5                | 19.8                |
      | **TANGO**        | -                    | -                   | -                       | -                   | 35.5                | 19.5                |
      | **DAgRL+OD**     | 38.5                 | 21.1                | 39.0                    | 21.4                | 37.1                | **19.9**            |
      | **Ours (MTU3D)**| **55.0**             | **23.6**            | **45.0**                | 14.7                | **40.8**            | 12.1                |

  - title: Qualitative results from Simulator
    text: |
        <div class="uk-grid-small uk-child-width-1-2 uk-child-width-1-2@m" uk-grid style="max-height:600px margin: 0 auto;">
          <div>
            <img src="qual-1.png" style="width:100%;  height:270px; object-fit:contain;">
            <span class="uk-text-meta">Object category goal</span>
          </div>
          <div>
            <img src="qual-2.png" style="width:100%; height:270px; object-fit:contain;">
            <span class="uk-text-meta">Language goal</span>
          </div>
          <div>
            <img src="qual-3.png" style="width:100%;  height:270px;  object-fit:contain;">
            <span class="uk-text-meta">Image goal</span>
          </div>
          <div>
            <img src="qual-4.png" style="width:100%;  height:270px;  object-fit:contain;">
            <span class="uk-text-meta">Task plan goal</span>
          </div>
        </div>

  - title: Real-world robot deployment
    text: |
          <div uk-slider>
            <div class="uk-slider-items uk-grid-small uk-child-width-1-1 uk-child-width-1-2@m uk-grid">
              <div>
                <video
                    src="demo-video1.mp4"
                    loop
                    muted
                    uk-video="autoplay:inview; playbackRate: 4"
                    /> 
              </div>
              <div>
                <video
                    src="demo-video2.mp4"
                    loop
                    muted
                    uk-video="autoplay:inview; playbackRate: 3"
                    /> 
              </div>
              <div>
                <video
                    src="demo-video3.mp4"
                    loop
                    muted
                    uk-video="autoplay:inview; playbackRate: 4"
                    /> 
              </div>
            </div>
            <div class="uk-flex uk-flex-center uk-margin-small">
              <a class="uk-icon-button" href uk-slidenav-previous uk-slider-item="previous"></a>
              <a class="uk-icon-button uk-margin-small-left" href uk-slidenav-next uk-slider-item="next"></a>
            </div>
          </div>

projects: # relevant projects
  - title: SQA3D
    description: A project for situated 3D spatial question answering.
    img: https://sqascannetviz.s3.us-west-1.amazonaws.com/sqa3d_website/teaser.JPG
    journal: "Research Project"
    url: https://sqa3d.github.io/
  - title: 3D-VisTA
    description: A Pre-trained transformer for 3D Vision and Textual Alignment.
    img: https://3d-vista.github.io/file/overall.png
    journal: "Research Project"
    url: https://3d-vista.github.io/
  - title: PQ3D
    description: A Unifed model for 3D Vision Language Understanding.
    img: https://pq3d.github.io/file/pq3d-logo.png
    journal: "Research Project"
    url: https://pq3d.github.io/
  - title: LEO
    description: Creating Embodied Generalist for 3D World.
    img: https://embodied-generalist.github.io/assets/leo.svg
    journal: "Research Project"
    url: https://embodied-generalist.github.io/
  - title: SceneVerse
    description: A Large Scale Dataset for 3D Vision Language Pre-training.
    img: https://scene-verse.github.io/assets/logo025.png
    journal: "Research Project"
    url: https://scene-verse.github.io/
  - title: SG3D
    description: A Benchmark for Task-oriented Sequential Grounding.
    img: https://sg-3d.github.io/assets/task_generation-1.png
    journal: "Research Project"
    url: https://sg-3d.github.io/
